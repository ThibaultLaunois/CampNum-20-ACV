{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACV Part 1: Image Manipulation and Transformation\n",
    "\n",
    "In this first part, we'll learn how to transform images at the pixel level using NumPy and OpenCV. By the end of this notebook, you'll be able to take a basic image and apply various transformations to it.\n",
    "\n",
    "<p style=\"display:flex; flex-flow:row nowrap; align-items:center; font-size:2rem\">\n",
    "From\n",
    "<img src=\"data/fish_base.jpg\" alt=\"base image\" width=\"256px\" style=\"margin:20px\"/>\n",
    "To \n",
    "<img src=\"nb_data/fish_toon.jpg\" alt=\"drawing\" width=\"256px\" style=\"margin:20px\"/>\n",
    "!\n",
    "</p> \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- **Understand image representation**: Learn how computers interpret and store images as arrays of numbers\n",
    "- **Master basic image operations**: Crop, fill, blend, and mask images using NumPy\n",
    "- **Manipulate pixel values**: Adjust contrast, color balance, and work with different color spaces\n",
    "- **Apply image filters**: Use convolutions to blur, sharpen, and detect edges in images\n",
    "- **Transform images spatially**: Resize, rotate, and warp images\n",
    "\n",
    "This knowledge will form the foundation for developing your own AI-based photobooth application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook setup**\n",
    "\n",
    "Don't change the cells in this section... except if you know what you're doing! (it is used to style some of the other cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".exercise {\n",
       "    color:#132743;\n",
       "    background:#AFE4FE;\n",
       "    padding:10px;\n",
       "    border-radius:10px;\n",
       "}\n",
       "\n",
       ".highlight {\n",
       "    color:#ff6f3c;\n",
       "    # background:#ff9a3c;\n",
       "    font-size: 1.3rem;\n",
       "    font-weight: bold;\n",
       "    font-style: italic;\n",
       "    text-align: center;\n",
       "    padding:10px;\n",
       "    border-radius:10px;\n",
       "}\n",
       "\n",
       ".theory-box {\n",
       "    color:#2E3440;\n",
       "    background:#E5E9F0;\n",
       "    padding:15px;\n",
       "    border-left: 5px solid #5E81AC;\n",
       "    border-radius:5px;\n",
       "    margin: 15px 0px;\n",
       "}\n",
       "\n",
       ".hint-box {\n",
       "    color:#2E3440;\n",
       "    background:#EBCB8B40;\n",
       "    padding:15px;\n",
       "    border-left: 5px solid #EBCB8B;\n",
       "    border-radius:5px;\n",
       "    margin: 15px 0px;\n",
       "}\n",
       "\n",
       ".pitfall-box {\n",
       "    color:#2E3440;\n",
       "    background:#BF616A30;\n",
       "    padding:15px;\n",
       "    border-left: 5px solid #BF616A;\n",
       "    border-radius:5px;\n",
       "    margin: 15px 0px;\n",
       "}\n",
       "\n",
       ".summary-box {\n",
       "    color:#2E3440;\n",
       "    background:#A3BE8C30;\n",
       "    padding:15px;\n",
       "    border-left: 5px solid #A3BE8C;\n",
       "    border-radius:5px;\n",
       "    margin: 15px 0px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".exercise {\n",
    "    color:#132743;\n",
    "    background:#AFE4FE;\n",
    "    padding:10px;\n",
    "    border-radius:10px;\n",
    "}\n",
    "\n",
    ".highlight {\n",
    "    color:#ff6f3c;\n",
    "    # background:#ff9a3c;\n",
    "    font-size: 1.3rem;\n",
    "    font-weight: bold;\n",
    "    font-style: italic;\n",
    "    text-align: center;\n",
    "    padding:10px;\n",
    "    border-radius:10px;\n",
    "}\n",
    "\n",
    ".theory-box {\n",
    "    color:#2E3440;\n",
    "    background:#E5E9F0;\n",
    "    padding:15px;\n",
    "    border-left: 5px solid #5E81AC;\n",
    "    border-radius:5px;\n",
    "    margin: 15px 0px;\n",
    "}\n",
    "\n",
    ".hint-box {\n",
    "    color:#2E3440;\n",
    "    background:#EBCB8B40;\n",
    "    padding:15px;\n",
    "    border-left: 5px solid #EBCB8B;\n",
    "    border-radius:5px;\n",
    "    margin: 15px 0px;\n",
    "}\n",
    "\n",
    ".pitfall-box {\n",
    "    color:#2E3440;\n",
    "    background:#BF616A30;\n",
    "    padding:15px;\n",
    "    border-left: 5px solid #BF616A;\n",
    "    border-radius:5px;\n",
    "    margin: 15px 0px;\n",
    "}\n",
    "\n",
    ".summary-box {\n",
    "    color:#2E3440;\n",
    "    background:#A3BE8C30;\n",
    "    padding:15px;\n",
    "    border-left: 5px solid #A3BE8C;\n",
    "    border-radius:5px;\n",
    "    margin: 15px 0px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image basics: NumPy vs OpenCV\n",
    "\n",
    "In this section, we'll explore how images are represented in computer memory and how to manipulate them using two powerful libraries: NumPy and OpenCV.\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Key Concepts</h3>\n",
    "<p><strong>Images as arrays</strong>: At their core, digital images are just multi-dimensional arrays of numbers. Each number represents the intensity of a pixel at a specific position and color channel.</p>\n",
    "\n",
    "**Resources**\n",
    "\n",
    "**OpenCV**:  \n",
    "- https://docs.opencv.org/4.6.0/\n",
    "- https://docs.opencv.org/4.6.0/d6/d00/tutorial_py_root.html\n",
    "\n",
    "**NumPy**:  \n",
    "- https://numpy.org/doc/stable/index.html  \n",
    "- https://www.w3resource.com/numpy/data-types.php  \n",
    "- https://numpy.org/doc/stable/user/absolute_beginners.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please create a new env and use pip to install the required libraries.\n",
    "# !pip install numpy opencv-python matplotlib plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we'll be using.\n",
    "import numpy as np      # For array operations\n",
    "import cv2              # OpenCV for computer vision functions\n",
    "import matplotlib.pyplot as plt  # For displaying images\n",
    "import plotly.graph_objects as go  # For interactive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read / Display / Write images\n",
    "\n",
    "Let's start by learning how to load, display, and save images using both Matplotlib and OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images using matplotlib - returns RGB format by default\n",
    "fish = plt.imread('data/fish_base.jpg')\n",
    "glasses = plt.imread('data/glasses.png')\n",
    "\n",
    "# Read using opencv - returns BGR format by default (note this difference for later!)\n",
    "cv_fish = cv2.imread('data/fish_base.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fish), type(glasses), type(cv_fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(fish)\n",
    "plt.title('Fish image loaded with Matplotlib')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(glasses)\n",
    "plt.title('Glasses image loaded with Matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image using OpenCV\n",
    "# This opens a separate window outside the notebook\n",
    "cv2.imshow('Fish image loaded with OpenCV', cv_fish)\n",
    "\n",
    "# DO NOT CLOSE THE WINDOW MANUALLY or you will need to restart the kernel, \n",
    "# just clic on the new window and press any key to close it\n",
    "\n",
    "cv2.waitKey(0)\n",
    "# Destroy the window\n",
    "cv2.destroyWindow('Fish image loaded with OpenCV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Understanding <code>cv2.waitKey(0)</code></h3>\n",
    "<p>The <code>waitKey()</code> function waits for a key event for a specified number of milliseconds:</p>\n",
    "<ul>\n",
    "    <li><code>waitKey(0)</code>: Waits indefinitely until any key is pressed</li>\n",
    "    <li><code>waitKey(1000)</code>: Waits for 1 second (1000 ms) or until a key is pressed, whichever comes first</li>\n",
    "</ul>\n",
    "<p>This function is essential when displaying images with OpenCV because it keeps the window open and gives control to the operating system to handle events.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to write an image to disk:\n",
    "\n",
    "# Using OpenCV (saves in BGR format)\n",
    "cv2.imwrite('mycvimage.png', cv_fish)\n",
    "\n",
    "# Using matplotlib (saves in RGB format)\n",
    "plt.imsave('myimage.png', fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.1</h4>\n",
    "<ul>\n",
    "    <li>What is the parameter \"0\" doing in the <code>waitKey()</code> function?</li>\n",
    "    <li>Complete the function below that listens for a specific key being pressed to close the window<br>\n",
    "    <em>(you can use the <a href=\"https://initialcommit.com/blog/python-ord-function\"><code>ord()</code></a> method to retrieve the Unicode value for any particular key):</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint</h4>\n",
    "<p>The <code>ord()</code> function converts a character to its Unicode code point (an integer). For example, <code>ord('q')</code> returns 113.</p>\n",
    "<p>You'll need to use a loop that continues until the specific key is pressed.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, window_title=\"HelloWorld\", key_close=\"q\"):\n",
    "    \"\"\" Display the image in an external window and wait for a specific keypress to close the window\n",
    "\n",
    "    Arguments:\n",
    "        img (np.ndarray): the image to display\n",
    "        window_title (string)[Optional]: Specify the title of the window. Defaults to \"HelloWorld\"\n",
    "        key_close (string)[Optional]: Specify the key that should be pressed to kill the window. Defaults to \"q\"\n",
    "    \"\"\"\n",
    "    # your code starts here\n",
    "\n",
    "    # your code ends here\n",
    "\n",
    "\n",
    "# run the function, the same image should be displayed as previously, but this time only 'q' should close the window\n",
    "show_image(cv_fish, window_title=\"Press 'q' to quit\", key_close=\"q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>Common Pitfall</h4>\n",
    "<p>A common mistake is to use <code>waitKey()</code> without checking its return value. Remember that <code>waitKey()</code> returns the ASCII value of the key pressed, or -1 if no key was pressed within the specified time.</p>\n",
    "<p>Also, on some systems, you may need to use <code>waitKey(0) & 0xFF</code> to extract only the relevant byte, especially when working with special keys.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image structure\n",
    "\n",
    "It is essential to understand how an image is interpreted by our computer programs. Let's begin by displaying some properties of the fish image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(fish): <class 'numpy.ndarray'>\n",
      "fish.dtype: uint8\n",
      "fish.shape = (534, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(fish): {type(fish)}\")  # The type of the image object\n",
    "print(f\"fish.dtype: {fish.dtype}\")  # The data type of the pixel values\n",
    "print(f\"fish.shape = {fish.shape}\") # The dimensions of the image (height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Image Structure</h3>\n",
    "<p>An image in computer memory is just a 3D array of numbers:</p>\n",
    "<ul>\n",
    "    <li><strong>First dimension (height)</strong>: Number of rows (from top to bottom)</li>\n",
    "    <li><strong>Second dimension (width)</strong>: Number of columns (from left to right)</li>\n",
    "    <li><strong>Third dimension (channels)</strong>: Color information (typically 3 for RGB or BGR)</li>\n",
    "</ul>\n",
    "<p>The <code>uint8</code> data type means \"unsigned integers stored on 8 bits (1 byte)\", ranging from 0 to 255. This is standard for everyday camera images.</p>\n",
    "<p>For example, a pixel value of [255, 0, 0] in RGB format represents pure red, while [0, 255, 0] represents pure green.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.2.1</h4>\n",
    "<ul>\n",
    "    <li>What is a pixel?</li>\n",
    "    <li>As has been done with the fish image, display the shape and the data type of the image with the glasses in the cell below. What do you observe?</li>\n",
    "    <li>What are the main differences between an image being stored in the <code>uint8</code> format vs the <code>float32</code> format?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code starts here\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>[SPOILER] Answers to Exercise 1.2.1</summary>\n",
    "<div class=\"theory-box\">\n",
    "<p><strong>What is a pixel?</strong><br>\n",
    "A pixel (short for \"picture element\") is the smallest unit of a digital image. It represents a single point in the image and contains color/intensity information. In a color image, each pixel typically has multiple values (channels) representing different color components (e.g., Red, Green, Blue).</p>\n",
    "\n",
    "<p><strong>Observation about glasses image:</strong><br>\n",
    "The glasses image has 4 channels instead of 3. The first 3 are RGB (color information), and the 4th channel is alpha (transparency information).</p>\n",
    "\n",
    "<p><strong>uint8 vs float32:</strong><br>\n",
    "<ul>\n",
    "    <li><strong>uint8</strong>: 8-bit unsigned integers (0-255)\n",
    "        <ul>\n",
    "            <li>Advantages: Memory efficient (1 byte per value), standard format for most image files</li>\n",
    "            <li>Limitations: Limited precision, can lead to rounding errors in calculations</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>float32</strong>: 32-bit floating-point numbers\n",
    "        <ul>\n",
    "            <li>Advantages: Higher precision, better for mathematical operations, can represent values outside 0-255 range</li>\n",
    "            <li>Limitations: Uses more memory (4 bytes per value), must be converted back to uint8 for display or saving</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "When performing image processing operations, it's often beneficial to convert to float32, do the calculations, then convert back to uint8 for display.</p>\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the image with the glasses has 4 channels (3rd value in the shape tuple). This 4th dimension contains information about `image transparency`:\n",
    "\n",
    "  * 0.0 --> completely transparent\n",
    "  * 1.0 --> opaque _(can also be 255 depending on the pixel's data type)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick experiment before moving to the next part : \n",
    "\n",
    "<div class=\"highlight\">\n",
    "    Are the <code>plt.imshow()</code> and <code>cv2.imshow()</code> methods the same?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we use pyplot to display the image imported with OpenCV (ie: `cv_fish`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cv_fish)\n",
    "plt.title('OpenCV image displayed with Matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened??!**\n",
    "\n",
    "<div class=\"pitfall-box\">\n",
    "<h4>RGB vs BGR Color Order</h4>\n",
    "<p>For historical reasons, OpenCV defaults to the BGR format instead of the usual RGB... the red and blue channels are inverted!</p>\n",
    "<p>When you load an image with <code>cv2.imread()</code>, it's in BGR format. But when you display it with <code>plt.imshow()</code>, Matplotlib expects RGB format.</p>\n",
    "<p>This is why the colors look wrong when displaying an OpenCV image with Matplotlib. Always be aware of which color format you're working with!</p>\n",
    "</div>\n",
    "\n",
    "As far as pyplot is concerned, it only sees arrays of numbers and doesn't mind plotting the disordered channels. Be careful with this as you might end up transforming the wrong channels if you're not careful! It's a good habit to display your transformed image at each step of the transformation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the channels are indeed inverted by plotting the first pixel value located at position (0,0) in the two images (the one in the top left corner):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cv2.imread(): cv_fish[0,0,:] = {cv_fish[0,0,:]}\t[B G R]\")\n",
    "print(f\"plt.imread(): fish[0,0,:]    = {fish[0,0,:]}\t[R G B]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.2.2</h4>\n",
    "Write a function to handle the image display for us and invert the RGB channels if necessary. Write an other function to diplay basic information about an image. \n",
    "\n",
    "USE THOSE FUNCTIONS AT EVERY STEP DURING YOUR WORK!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to copy this function in a cell at the top of the notebook\n",
    "# in case you have to restart the kernel and reload the entire notebook\n",
    "\n",
    "def display_image(image, convert_bgr_to_rgb=False):\n",
    "    \"\"\" Display the image using matplotlib\n",
    "\n",
    "    Arguments:\n",
    "        image (np.ndarray): the image to display\n",
    "        convert_bgr_to_rgb (bool)[Optional]: Set this to True when image comes from OpenCV imread\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    \n",
    "def image_info(image):\n",
    "    \"\"\" Display basic information about an image\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): the image to analyze\n",
    "    \"\"\"\n",
    "    print('Image type:', type(image))\n",
    "    print('Image data type:', image.dtype)\n",
    "    print('Image shape:', image.shape)\n",
    "    print('Min value:', image.min())\n",
    "    print('Max value:', image.max())\n",
    "    \n",
    "# Test our functions\n",
    "image_info(fish)\n",
    "display_image(cv_fish, True)  # Convert BGR to RGB for proper display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section Summary Image Basics</h3>\n",
    "<p>In this section, we've learned the fundamentals:</p>\n",
    "<ul>\n",
    "    <li>How to read, display, and write images using both Matplotlib and OpenCV</li>\n",
    "    <li>The structure of digital images as multi-dimensional arrays</li>\n",
    "    <li>The difference between RGB (Matplotlib) and BGR (OpenCV) color formats</li>\n",
    "    <li>How to handle image display with proper color conversion</li>\n",
    "    <li>The importance of understanding image data types and their implications</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Crop, Fill, Blend, Mask\n",
    "\n",
    "Now that we understand the basics of image representation, let's learn how to perform fundamental image operations using NumPy array manipulation. These operations form the building blocks for more complex image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the images (in case you have to reload the notebook)\n",
    "fish = plt.imread('data/fish_base.jpg')\n",
    "glasses = plt.imread('data/glasses.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Basic Image Operations</h3>\n",
    "<p>We will see four fundamental operations we can perform on images using NumPy:</p>\n",
    "<ul>\n",
    "    <li><strong>Crop</strong>: Extract a portion of an image using array slicing</li>\n",
    "    <li><strong>Fill</strong>: Replace pixel values in specific regions</li>\n",
    "    <li><strong>Blend</strong>: Combine two images with different weights</li>\n",
    "    <li><strong>Mask</strong>: Use a binary or grayscale mask to selectively transform an image</li>\n",
    "</ul>\n",
    "<p>These operations leverage NumPy's powerful array manipulation capabilities and form the foundation for more complex image processing techniques.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.3</h4>\n",
    "<strong>Crop:</strong> Display only a 256x256 pixel area in the center of the fish image<br>\n",
    "<img src=\"./nb_data/cropped_fish.jpg\" style=\"width:100px; margin:10px\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Image Cropping</h4>\n",
    "<p>To crop an image in NumPy, you need to:</p>\n",
    "<ol>\n",
    "    <li>Determine the center point of the image</li>\n",
    "    <li>Calculate the starting and ending indices for your crop</li>\n",
    "    <li>Use array slicing to extract the region</li>\n",
    "</ol>\n",
    "<p>Remember that images are indexed as [height, width, channels], so you'll need to calculate the crop boundaries for both height and width.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the cropped area\n",
    "px = 256\n",
    "half_px = px//2  # integer division, also called \"floor division\" as it floors the result of the division \n",
    "                 # -> very important to keep in mind when working with pixels!\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "# your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Array Slicing for Image Cropping</h3>\n",
    "<p>When we use <code>fish[half_height-half_px : half_height+half_px, half_width-half_px : half_width+half_px, :]</code>, we're performing array slicing in three dimensions:</p>\n",
    "<ul>\n",
    "    <li>First dimension (height): From <code>half_height-half_px</code> to <code>half_height+half_px</code></li>\n",
    "    <li>Second dimension (width): From <code>half_width-half_px</code> to <code>half_width+half_px</code></li>\n",
    "    <li>Third dimension (channels): All channels (indicated by <code>:</code>)</li>\n",
    "</ul>\n",
    "<p>This creates a new array containing only the specified region of the original image.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.4</h4>\n",
    "<strong>Fill:</strong> Draw a big red \"+\" sign on the fish image.<br>\n",
    "<img src=\"./nb_data/plus_fish.jpg\" style=\"width:200px; margin:10px\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Image Filling</h4>\n",
    "<p>To draw a \"+\" sign, you need to:</p>\n",
    "<ol>\n",
    "    <li>Create a copy of the original image to avoid modifying it</li>\n",
    "    <li>Define the dimensions of the horizontal and vertical bars</li>\n",
    "    <li>Use array slicing to replace pixel values in those regions</li>\n",
    "</ol>\n",
    "<p>Remember that RGB colors are represented as [R, G, B] arrays, so [255, 50, 50] would be a bright red color.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original image to avoid modifying it\n",
    "plus_fish = fish.copy()\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "# your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>Common Pitfall: Modifying Original Images</h4>\n",
    "<p>When working with images in NumPy, remember that assignment operations modify the array in-place. If you don't want to modify your original image, always create a copy first with <code>image.copy()</code>.</p>\n",
    "<p>This is especially important when you're experimenting with different transformations or need to reuse the original image later.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.5</h4>\n",
    "<strong>Blend:</strong> Display the result of mixing 30% <code>upscale_rgb_glasses</code> to 70% <code>fish</code> images.<br>\n",
    "You will first need to overcome the shape mismatch between the <code>fish</code> and the <code>glasses</code> images, before you can combine them.<br>\n",
    "<img src=\"./nb_data/glasses_fish.jpg\" style=\"width:200px; margin:10px\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Image Blending</h3>\n",
    "<p>Image blending combines two images using a weighted average:</p>\n",
    "<p><code>blended_image = α × image1 + (1-α) × image2</code></p>\n",
    "<p>where α is a value between 0 and 1 that determines the contribution of each image.</p>\n",
    "<p>Before blending, both images must have the same dimensions. If they don't, we need to resize or pad one of them to match the other.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code for casting the fish and glasses image to the same shape here (name the new glasses image upscale_rgb_glasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both images with the same dimensions, we can blend them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for combining both images here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>Potential Issue: Integer Overflow</h4>\n",
    "<p>When blending images, be careful about potential overflow. If you add pixel values that sum to more than 255 (or 1 if type is float), you'll get unexpected results due to overflow.</p>\n",
    "<p>To avoid this, you can use <code>np.clip(result, 0, 255)</code> to ensure values stay in the valid range.</li>\n",
    "</ol>\n",
    "<p>In our example, we're safe because 0.7*255 + 0.3*255 = 255, but with different weights, this could be an issue.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.6</h4>\n",
    "<strong>Mask:</strong> Use an <code>upscale_alpha_glasses</code> image as weights to compute the weighted mean of <code>upscale_rgb_glasses</code> and <code>fish</code>. This operation is often referred to as <code>alpha_blending</code>.<br>\n",
    "<img src=\"./nb_data/masked_fish.jpg\" style=\"width:200px; margin:10px\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Alpha Blending</h3>\n",
    "<p>Alpha blending is a more sophisticated form of image blending that uses a transparency mask (the alpha channel) to determine how much each image contributes to each pixel:</p>\n",
    "<p><code>result = foreground × alpha + background × (1-alpha)</code></p>\n",
    "<p>where <code>alpha</code> is a value between 0 and 1 for each pixel:</p>\n",
    "<ul>\n",
    "    <li>alpha = 1: Use only the foreground pixel</li>\n",
    "    <li>alpha = 0: Use only the background pixel</li>\n",
    "    <li>alpha = 0.5: Equal mix of foreground and background</li>\n",
    "</ul>\n",
    "<p>This allows for smooth transitions and partial transparency effects.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results and make sure that all the images have the same size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the alpha blending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Vectorized Operations</h4>\n",
    "<p>The channel-by-channel approach above works, but FYI, NumPy offers a more concise way using broadcasting:</p>\n",
    "<pre>\n",
    "masked_fish = (fish * (1-upscale_alpha_glasses[:,:,np.newaxis]) + \n",
    "               upscale_rgb_glasses * upscale_alpha_glasses[:,:,np.newaxis]).astype(np.uint8)\n",
    "</pre>\n",
    "<p>This adds a new axis to the alpha channel to make it compatible with the 3-channel images, then performs the blending in a single operation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 1.7</h4>\n",
    "Turn the previous code that does the alpha blending of two images into a function (you may need this for your photoboot project!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_image(imageA: np.ndarray, imageB: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Blend two images using an alpha mask\n",
    "    \n",
    "    Arguments:\n",
    "        imageA (np.ndarray): Background image\n",
    "        imageB (np.ndarray): Foreground image\n",
    "        mask (np.ndarray): Alpha mask with values in range [0, 1]\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The blended image\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "\n",
    "# Test the function\n",
    "result = composite_image(fish, upscale_rgb_glasses, upscale_alpha_glasses)\n",
    "display_image(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding <code>np.newaxis</code> and Ellipsis (<code>...</code>)</h3>\n",
    "<p><code>np.newaxis</code> is used to add a new dimension to an array. In our function, we use it to make the 2D mask compatible with the 3D images.</p>\n",
    "<p>The ellipsis (<code>...</code>) is a shorthand that means \"all existing dimensions\". So <code>mask[..., np.newaxis]</code> means \"keep all existing dimensions of mask and add a new one at the end\".</p>\n",
    "<p>This is equivalent to <code>mask[:,:,np.newaxis]</code> for a 2D mask, but the ellipsis notation is more flexible as it works regardless of how many dimensions the input has.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section summary: Basic Image Operations</h3>\n",
    "<p>In this section, we've learned how to:</p>\n",
    "<ul>\n",
    "    <li><strong>Crop</strong> images by selecting specific regions using array slicing</li>\n",
    "    <li><strong>Fill</strong> regions of an image with specific colors</li>\n",
    "    <li><strong>Blend</strong> two images using weighted averaging</li>\n",
    "    <li><strong>Use the alpha channel</strong> and alpha blending for smooth transitions</li>\n",
    "</ul>\n",
    "<p>These fundamental operations can be combined in various ways to create more complex image manipulations and effects.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manipulating Pixel Values\n",
    "\n",
    "In this section, we'll explore how to modify the actual values of pixels to enhance images and create interesting effects. By the end of this section, you'll be able to create a composite image of the fish wearing glasses with:\n",
    "\n",
    "- \"Improved\" (that may vary from your pov) image quality of the fish\n",
    "- Glasses with infinite colors using hue shifting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the images (in case you have to reload the notebook)\n",
    "fish = plt.imread('data/fish_base.jpg')\n",
    "glasses = plt.imread('data/glasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for channel manipulation\n",
    "def split_channels(image):\n",
    "    \"\"\" Split an image into its individual channels\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image to split\n",
    "        \n",
    "    Returns:\n",
    "        generator: A generator yielding each channel\n",
    "    \"\"\"\n",
    "    return (image[..., i] for i in range(image.shape[-1]))\n",
    "\n",
    "# Utility function that helps you to recompose an image from a list of channels\n",
    "def recompose_channels(chan_list):\n",
    "    \"\"\" Combine individual channels back into an image\n",
    "    \n",
    "    Arguments:\n",
    "        chan_list (list): List of channels to combine\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The recomposed image\n",
    "    \"\"\"\n",
    "    return np.stack(chan_list, axis=2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 White balance\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding White Balance</h3>\n",
    "<p>White balance is the process of removing unrealistic color casts from images, so that objects that appear white in person are rendered white in your photo.</p>\n",
    "<p>Different light sources (sunlight, fluorescent, incandescent) emit light with different color temperatures, which can cause color casts in photos. White balancing adjusts the relative strengths of color channels to compensate for these casts.</p>\n",
    "<p>A common approach is to find an area in the image that should be neutral gray or white, and then adjust the color channels so that this area has equal RGB values.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 2.1</h4>\n",
    "Apply white balancing to the fish image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: White Balance Approach</h4>\n",
    "<p>To perform white balance:</p>\n",
    "<ol>\n",
    "    <li>Select a region that should be neutral (like the fish's teeth)</li>\n",
    "    <li>Calculate the average value of each color channel in this region</li>\n",
    "    <li>Compute scaling factors to make these averages equal</li>\n",
    "    <li>Apply these scaling factors to the entire image</li>\n",
    "</ol>\n",
    "<p>A common approach is to use the green channel as a reference and adjust red and blue to match it.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_balance(image):\n",
    "    \"\"\" Return the red and blue gain, computed with respect to the green.\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image region to analyze for white balance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (gain_r, gain_b) scaling factors for red and blue channels\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    " \n",
    "    \n",
    "    return gain_r, gain_b\n",
    "\n",
    "def apply_white_balance(image, gain_r, gain_b):\n",
    "    \"\"\"Apply gain_r and gain_b to the red and blue channels respectively\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image to white balance\n",
    "        gain_r (float): Scaling factor for the red channel\n",
    "        gain_b (float): Scaling factor for the blue channel\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The white-balanced image\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "  \n",
    "    \n",
    "    return balanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply and display the results of the white balance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... As you can see, not a lot has changed between the two images, at least not visually, that's because the white balance in this case is quite small (you should find something like `gain_r=0.98` and `gain_b=1.01` for the red and the blue balance respectively).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Subtle vs. Dramatic White Balance</h3>\n",
    "<p>In the example above, the white balance adjustment is quite subtle because the original image was already fairly well balanced. The gain factors are close to 1.0, indicating only minor adjustments were needed.</p>\n",
    "<p>In real-world photography, white balance corrections can be much more dramatic, especially when dealing with extreme lighting conditions like sunset (very orange) or fluorescent lighting (often greenish).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be reassured that everything works as expected let's artificially crank up the blue balance a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force more dramatic \"white balance\" adjustments\n",
    "gain_r_forced = 1.0\n",
    "gain_b_forced = 1.5  # Increase blue channel by 50%\n",
    "\n",
    "# Display the forced white balance alongside the normal image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it works! Don't hesitate to play around with the values a bit.  \n",
    "\n",
    "From now on we'll use the balanced image as our new default fish image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the balanced image as our new default fish image for next steps\n",
    "fish = apply_white_balance(fish, gain_r, gain_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Histogram of channels values\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Image Histograms</h3>\n",
    "<p>An image histogram is a graphical representation of the distribution of pixel values in an image. For a color image, we typically create separate histograms for each color channel.</p>\n",
    "<p>Histograms provide valuable information about:</p>\n",
    "<ul>\n",
    "    <li><strong>Brightness</strong>: Right-skewed histograms indicate bright images, left-skewed indicate dark images</li>\n",
    "    <li><strong>Contrast</strong>: Narrow histograms indicate low contrast, wide histograms indicate high contrast</li>\n",
    "    <li><strong>Color balance</strong>: Comparing histograms across channels can reveal color casts</li>\n",
    "    <li><strong>Clipping</strong>: Spikes at 0 or 255 indicate loss of detail in shadows or highlights</li>\n",
    "</ul>\n",
    "<p>Histograms are essential tools for diagnosing image quality issues and guiding adjustments.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 2.2</h4>\n",
    "Plot the distribution of the pixel values for each individual channel in the balanced fish image in a histogram.<br>\n",
    "    <ul>\n",
    "        <li><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.histogram.html\">numpy.histogram()</a></li>\n",
    "    </ul>\n",
    "Looking at this histogram, what do you think could be improved?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Creating Histograms</h4>\n",
    "<p>To create a histogram of pixel values:</p>\n",
    "<ol>\n",
    "    <li>Split the image into its color channels</li>\n",
    "    <li>For each channel, use <code>np.histogram()</code> to count pixels in each intensity bin</li>\n",
    "    <li>Plot the results using matplotlib or plotly</li>\n",
    "</ol>\n",
    "<p>When analyzing the histogram, look for:</p>\n",
    "<ul>\n",
    "    <li>Whether the full range of values (0-255) is being utilized</li>\n",
    "    <li>If there are any gaps or spikes in the distribution</li>\n",
    "    <li>Whether the distribution is balanced across all channels</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by writing a function that computes the histogram values of each channel from a predefined number of bins.\n",
    "# Add an extra option that enables the user to retrieve a normalized distribution\n",
    "\n",
    "def compute_histogram(image, normalize=False):\n",
    "    \"\"\" Compute histograms for each channel of an image\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image to analyze\n",
    "        normalize (bool): Whether to normalize the histogram (density=True)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bins, hist_r, hist_g, hist_b) histograms for each channel\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    return bins[:-1], r_hist, g_hist, b_hist  # Return bins[:-1] to match the length of the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the histogram, we recommend using plotly for this as it provides increased interactivity out of the box!\n",
    "def plot_histogram(image, normalize=False, show=True):\n",
    "    \"\"\" Plot histograms for each channel of an image\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image to analyze\n",
    "        normalize (bool): Whether to normalize the histogram\n",
    "        show (bool): Whether to display the plot\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram using matplotlib (simpler but less interactive)\n",
    "plot_histogram(fish, normalize=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram using plotly (more interactive)\n",
    "plot_histogram(fish, normalize=False, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Histogram Analysis</h3>\n",
    "<p>Looking at the histogram of our fish image, we can observe:</p>\n",
    "<ul>\n",
    "    <li>The pixel values don't utilize the full range from 0 to 255</li>\n",
    "    <li>Most values are concentrated in the middle range (around 50-150)</li>\n",
    "    <li>There's very little data in the highlights (values close to 255)</li>\n",
    "    <li>The distribution is similar across all three channels, which is good for color balance</li>\n",
    "</ul>\n",
    "<p>This suggests that the image could benefit from <strong>contrast enhancement</strong> to stretch the histogram to utilize the full range of values, which would make the image appear more vibrant.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Playing with the image Gain, Offset and Contrast\n",
    "\n",
    "An easy and straightforward way to improve the contrast is to offset the pixel values by the minimum, and scale everything so that the values cover the entire 0-255 range of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 2.3</h4>\n",
    "Improve the contrast of the fish image!<br>\n",
    "    <ul>\n",
    "        <li><a href=\"https://en.wikipedia.org/wiki/Contrast_(vision)\">What is contrast?</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Contrast</h3>\n",
    "<p>Contrast refers to the difference in luminance or color that makes objects distinguishable from one another and their background.</p>\n",
    "<p>In image processing, contrast enhancement typically involves:</p>\n",
    "<ol>\n",
    "    <li><strong>Normalization</strong>: Shifting the minimum value to 0</li>\n",
    "    <li><strong>Stretching</strong>: Scaling the range to utilize the full available spectrum (0-255 for 8-bit images)</li>\n",
    "</ol>\n",
    "<p>The formula for linear contrast stretching is:</p>\n",
    "<p><code>new_value = (old_value - old_min) * (new_max - new_min) / (old_max - old_min) + new_min</code></p>\n",
    "<p>Where <code>new_min</code> is typically 0 and <code>new_max</code> is typically 255 for 8-bit images.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_contrast(image):\n",
    "    \"\"\" Apply simple contrast enhancement to an image\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The image to enhance\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The contrast-enhanced image\n",
    "    \"\"\"\n",
    "    # Your code starts here\n",
    "    # Your code ends here\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "# Apply contrast enhancement\n",
    "contrasted_fish = naive_contrast(fish)\n",
    "\n",
    "# Verify the maximum value is now 255\n",
    "print(f\"Minimum value after contrast enhancement: {np.min(contrasted_fish)}\")\n",
    "print(f\"Maximum value after contrast enhancement: {np.max(contrasted_fish)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display the results!\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(fish)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].set_axis_off()\n",
    "\n",
    "axes[1].imshow(contrasted_fish)\n",
    "axes[1].set_title(\"Contrast Enhanced Image\")\n",
    "axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's also compare the histograms\n",
    "plot_histogram(contrasted_fish, normalize=False, use_matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Histogram Analysis After Contrast Enhancement</h3>\n",
    "<p>Looking at the histogram after contrast enhancement, we can observe:</p>\n",
    "<ul>\n",
    "    <li>The pixel values now span the full range from 0 to 255</li>\n",
    "    <li>The overall shape of the distribution is preserved, but stretched</li>\n",
    "    <li>The image appears more vibrant with better distinction between dark and light areas</li>\n",
    "</ul>\n",
    "<p>This linear contrast enhancement is just one of many possible transformations we can apply to pixel values. Next, we'll explore some non-linear transformations.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Let's now talk about other color spaces (HSL, HSV, etc.)\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Color Spaces</h3>\n",
    "<p>A color space is a specific organization of colors that allows for reproducible representations of color. Different color spaces have different properties that make them useful for different applications:</p>\n",
    "<ul>\n",
    "    <li><strong>RGB (Red, Green, Blue)</strong>: Additive color model used in digital displays. Each pixel is represented by the amount of red, green, and blue light it emits.</li>\n",
    "    <li><strong>BGR</strong>: Same as RGB but with the order of channels reversed (used by OpenCV for historical reasons).</li>\n",
    "    <li><strong>HSV (Hue, Saturation, Value)</strong>: Represents colors in terms of:\n",
    "        <ul>\n",
    "            <li><em>Hue</em>: The color type (red, yellow, green, etc.) - represented as an angle (0-360°) (180° in OpenCV!)</li>\n",
    "            <li><em>Saturation</em>: The intensity or purity of the color (0-100%)</li>\n",
    "            <li><em>Value</em>: The brightness of the color (0-100%)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>HSL (Hue, Saturation, Lightness)</strong>: Similar to HSV but with a different approach to brightness.</li>\n",
    "    <li><strong>Grayscale</strong>: Single-channel representation of brightness.</li>\n",
    "</ul>\n",
    "<p>HSV and HSL are particularly useful for image processing because they separate color information (hue) from intensity information (saturation and value/lightness), making it easier to manipulate colors in intuitive ways.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV offers a convenient [`cvtColors`](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html) function to switch between color spaces. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Many algorithms operate on grayscale images as they are easier to work with (we get rid of the different RGB channels)\n",
    "gray_fish = cv2.cvtColor(fish, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(gray_fish)\n",
    "plt.title('Grayscale Fish (with default colormap)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>Colormap Confusion</h4>\n",
    "<p>The fancy colors in the grayscale image above are just a colormap applied by matplotlib to help visualize the intensity values. This is not the actual grayscale image!</p>\n",
    "<p>When displaying grayscale images, it's usually better to specify a grayscale colormap to avoid this confusion.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(gray_fish, cmap='gray')\n",
    "plt.title('Grayscale Fish (with gray colormap)')\n",
    "print(f\"Shape of grayscale image: {gray_fish.shape}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Grayscale Conversion</h3>\n",
    "<p>When converting a color image to grayscale, the RGB values are weighted to account for human perception of brightness:</p>\n",
    "<p><code>Gray = 0.299 × Red + 0.587 × Green + 0.114 × Blue</code></p>\n",
    "<p>This formula reflects the fact that human eyes are more sensitive to green light than red or blue.</p>\n",
    "<p>Notice that the grayscale image has only 2 dimensions (height and width) instead of 3, as the color information has been reduced to a single intensity value per pixel.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 2.4</h4>\n",
    "Let's try another color space: convert the image to HSV using another method from OpenCV<br>\n",
    "    <ul>\n",
    "        <li><a href=\"https://en.wikipedia.org/wiki/HSL_and_HSV\">HSL and HSV</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Color Space Conversion</h4>\n",
    "<p>To convert between color spaces in OpenCV, use the <code>cv2.cvtColor()</code> function with the appropriate conversion code:</p>\n",
    "<ul>\n",
    "    <li><code>cv2.COLOR_RGB2HSV</code>: Convert from RGB to HSV</li>\n",
    "    <li><code>cv2.COLOR_BGR2HSV</code>: Convert from BGR to HSV</li>\n",
    "    <li><code>cv2.COLOR_HSV2RGB</code>: Convert from HSV to RGB</li>\n",
    "</ul>\n",
    "<p>Remember to use the correct conversion code based on the current format of your image!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "# Convert the fish image from RGB to HSV\n",
    "\n",
    "# Your code ends here\n",
    "\n",
    "# Display the HSV image (note: this will look strange because matplotlib expects RGB)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(hsv_fish)\n",
    "plt.title('HSV Fish (displayed as if it were RGB)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>HSV Display Issues</h4>\n",
    "<p>The HSV image looks strange when displayed directly because matplotlib interprets the channels as RGB. This is not a problem with the conversion, but with how the data is being displayed.</p>\n",
    "<p>To properly visualize HSV images, it's better to look at each channel separately or convert back to RGB for display.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the hue, saturation and value separately!\n",
    "h, s, v = hsv_fish[:,:,0], hsv_fish[:,:,1], hsv_fish[:,:,2]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Display each channel with appropriate colormaps\n",
    "axes[0].imshow(h, cmap='hsv')  # Use hsv colormap for hue\n",
    "axes[0].set_title('Hue')\n",
    "axes[0].set_axis_off()\n",
    "\n",
    "axes[1].imshow(s, cmap='gray')  # Use grayscale for saturation\n",
    "axes[1].set_title('Saturation')\n",
    "axes[1].set_axis_off()\n",
    "\n",
    "axes[2].imshow(v, cmap='gray')  # Use grayscale for value\n",
    "axes[2].set_title('Value (~= Luminosity)')\n",
    "axes[2].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding HSV Channels</h3>\n",
    "<ul>\n",
    "    <li><strong>Hue (H)</strong>: Represents the color type. In OpenCV, it ranges from 0 to 180 (not 360 as in the standard definition).\n",
    "    <li><strong>Saturation (S)</strong>: Represents the purity of the color. Low values make the color look faded or grayish, while high values make the color more vibrant.</li>\n",
    "    <li><strong>Value (V)</strong>: Represents the brightness. Low values make the color dark, while high values make it bright.</li>\n",
    "</ul>\n",
    "<p>Common hue values in OpenCV's range (0-180):</p>\n",
    "<ul>\n",
    "    <li>Red: 0-30 or 150-180</li>\n",
    "    <li>Yellow: 30-60</li>\n",
    "    <li>Green: 60-90</li>\n",
    "    <li>Cyan: 90-120</li>\n",
    "    <li>Blue: 120-150</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 2.5</h4>\n",
    "<ol>\n",
    "<li>Use the <code>glasses</code> image (only the RGB component) and change its color by shifting the hue from green to ... pink?! Don't forget to convert the image back to the RGB space so it gets displayed properly.<br>\n",
    "<strong>Bonus:</strong> Instead of changing the color of the entire image, only change the color of the glasses' frame and keep the lenses as they are.</li><br>\n",
    "<li>Increase the color saturation of the fish image so you obtain something like this:<br>\n",
    "<img src=\"./nb_data/flashy_fish.jpg\" alt=\"flashy fish\" style=\"margin:10px; width:200px\"> </li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"border-top:1px solid black; margin-top:2rem; padding:1rem\">\n",
    "Might turn out handy: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.clip.html#numpy.clip\">numpy.clip()</a>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Hue Shifting</h4>\n",
    "<p>To shift the hue of an image:</p>\n",
    "<ol>\n",
    "    <li>Convert the image to HSV color space</li>\n",
    "    <li>Add a value to the H channel (remember to use modulo or clip() to wrap around if it exceeds 180)</li>\n",
    "    <li>Convert back to RGB for display</li>\n",
    "</ol>\n",
    "<p>For the bonus, you can use the alpha channel as a mask to determine which parts to modify.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by changing the color of the glasses\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the glasses image\n",
    "glasses = plt.imread('data/glasses.png')\n",
    "cv_glasses = cv2.imread('data/glasses.png')\n",
    "rgb_glasses = cv2.cvtColor(cv_glasses[:,:,:3], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Bonus: Selective Hue Shifting</h3>\n",
    "<p>To change only the frame color while keeping the lenses unchanged, you could:</p>\n",
    "<ol>\n",
    "    <li>Use the alpha channel to identify the non-transparent parts</li>\n",
    "    <li>Create a mask for the lenses (e.g., by color thresholding)</li>\n",
    "    <li>Apply the hue shift only to pixels that are non-transparent AND not part of the lenses</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's increase the saturation of the fish image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the fish image\n",
    "cv_fish = cv2.imread('data/fish_base.jpg')\n",
    "rgb_fish = cv2.cvtColor(cv_fish, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Your code starts here\n",
    "# 1. Convert the RGB fish to HSV\n",
    "hsv_fish = cv2.cvtColor(rgb_fish, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# 2. Extract the H, S, V channels\n",
    "h, s, v = hsv_fish[:,:,0], hsv_fish[:,:,1], hsv_fish[:,:,2]\n",
    "\n",
    "# 3. Increase the saturation (multiply by 2.0)\n",
    "s_increased = s * 2.0\n",
    "\n",
    "# 4. Clip values to ensure they stay in the valid range (0-255)\n",
    "s_increased = np.clip(s_increased, 0, 255).astype(np.uint8)\n",
    "\n",
    "# 5. Recombine the channels\n",
    "hsv_saturated = np.stack([h, s_increased, v], axis=2).astype(np.uint8)\n",
    "\n",
    "# 6. Convert back to RGB for display\n",
    "saturated_fish = cv2.cvtColor(hsv_saturated, cv2.COLOR_HSV2RGB)\n",
    "# Your code ends here\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes[0].imshow(rgb_fish)\n",
    "axes[0].set_title(\"Original Fish\")\n",
    "axes[0].set_axis_off()\n",
    "\n",
    "axes[1].imshow(saturated_fish)\n",
    "axes[1].set_title(\"Saturated Fish\")\n",
    "axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Saturation Adjustment</h3>\n",
    "<p>Increasing saturation makes colors more vibrant and intense, while decreasing it makes colors more muted and gray.</p>\n",
    "<p>Setting S to 0 everywhere would create a grayscale image (with the brightness determined by the V channel)<p>\n",
    "<p>Always remember use modulo and to clip values to the valid range (0-255) after multiplication to avoid overflow.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Complete the following function that makes any image more 'flashy'.\n",
    "# You can start with the following parameters:\n",
    "#   - increase the image saturation to 170%\n",
    "#   - increase the image value to 150%\n",
    "# Of course you can also add saturation and value as kwargs to the function to make it easier\n",
    "# to play with the values later (don't forget to update the documentation if you do so)\n",
    "\n",
    "def make_flashy(img, saturation_factor=1.7, value_factor=1.5):\n",
    "    \"\"\" This function enhances the contrast, saturation, and value of an image to make it more \"flashy\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        The image that should be processed.\n",
    "    saturation_factor : float, optional\n",
    "        Factor to multiply the saturation by. Default is 1.7.\n",
    "    value_factor : float, optional\n",
    "        Factor to multiply the value (brightness) by. Default is 1.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        The processed image with enhanced contrast, saturation, and value.\n",
    "    \"\"\"\n",
    "    # Your code starts here\n",
    "\n",
    "    # Your code ends here\n",
    "    \n",
    "    return flashy_img\n",
    "\n",
    "# Apply the function to the fish image\n",
    "flashy_fish = make_flashy(rgb_fish)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(flashy_fish)\n",
    "plt.title('Flashy Fish')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section summary: Manipulating Pixel Values</h3>\n",
    "<p>In this section, we've explored various ways to manipulate pixel values to enhance images and create interesting effects:</p>\n",
    "<ul>\n",
    "    <li><strong>White Balance</strong>: Adjusting color channels to correct for lighting conditions</li>\n",
    "    <li><strong>Histograms</strong>: Analyzing the distribution of pixel values to guide adjustments</li>\n",
    "    <li><strong>Contrast Enhancement</strong>: Stretching the range of pixel values to improve visibility</li>\n",
    "    <li><strong>Color Space Conversion</strong>: Working in different color spaces (RGB, grayscale, HSV)</li>\n",
    "    <li><strong>Hue Shifting</strong>: Changing colors while preserving structure</li>\n",
    "    <li><strong>Saturation Adjustment</strong>: Making colors more vibrant or muted</li>\n",
    "</ul>\n",
    "<p>These techniques form the foundation for more advanced image processing operations that we'll explore in the next sections.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Filtering and Convolutions\n",
    "\n",
    "In this section, we'll explore image filtering and convolutions, which are fundamental techniques in computer vision and image processing. These operations allow us to extract features, reduce noise, detect edges, and apply various effects to images.\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>What are Convolutions?</h3>\n",
    "<p>A convolution is a mathematical operation that combines two functions to produce a third function. In image processing, we apply a small matrix (called a kernel or filter) to each pixel of an image and its neighbors to produce a new value.</p>\n",
    "\n",
    "<p>The process works by:</p>\n",
    "<ol>\n",
    "    <li>Placing the kernel over a section of the image</li>\n",
    "    <li>Multiplying each kernel value by the corresponding image pixel</li>\n",
    "    <li>Summing these products to get the new pixel value</li>\n",
    "    <li>Moving the kernel to the next position (sliding window)</li>\n",
    "</ol>\n",
    "\n",
    "<p>Different kernels produce different effects, such as blurring, sharpening, or edge detection.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many `features` can be extracted from images using convolutions, just by tweaking the weights and size of the kernels. The 'goal' of Convolutional Neural Network (CNN) is basically to optimize these kernel weights to extract useful features from the image for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Low Pass Filters (Blurring)\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Low Pass Filters</h3>\n",
    "<p>Low pass filters allow low-frequency components of an image to pass through while attenuating high-frequency components. In simpler terms, they smooth out rapid changes in pixel values, which results in blurring.</p>\n",
    "\n",
    "<p>Common applications include:</p>\n",
    "<ul>\n",
    "    <li>Noise reduction</li>\n",
    "    <li>Removing fine details</li>\n",
    "    <li>Pre-processing for other operations</li>\n",
    "</ul>\n",
    "\n",
    "<p>The simplest low pass filter is a box filter (or averaging filter), where each pixel is replaced by the average of its neighborhood.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple kernel: a box filter of width 31\n",
    "K = np.ones((31, 1), dtype=float)\n",
    "K /= K.sum()  # Normalize the kernel so the sum of all elements is 1\n",
    "\n",
    "# Apply vertical filter\n",
    "filtered_v = cv2.filter2D(fish, -1, K)\n",
    "\n",
    "# Apply horizontal filter (transpose kernel)\n",
    "filtered_h = cv2.filter2D(fish, -1, K.T)\n",
    "\n",
    "# Apply both filters in different ways\n",
    "# Method 1: Use the outer product of the kernels\n",
    "filtered_hv0 = cv2.filter2D(fish, -1, K.dot(K.T))\n",
    "\n",
    "# Method 2: Apply horizontal filter, then vertical filter\n",
    "filtered_hv1 = cv2.filter2D(filtered_h, -1, K)\n",
    "\n",
    "# Method 3: Apply vertical filter, then horizontal filter\n",
    "filtered_hv2 = cv2.filter2D(filtered_v, -1, K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].imshow(fish)\n",
    "axes[1].imshow(filtered_v)\n",
    "axes[2].imshow(filtered_h)\n",
    "axes[3].imshow(filtered_hv0)\n",
    "axes[4].imshow(filtered_hv1)\n",
    "axes[5].imshow(filtered_hv2)\n",
    "\n",
    "titles = [\n",
    "    \"Original Fish\", \"Vertical Blur\", \"Horizontal Blur\",\n",
    "    \"2D Kernel Blur\", \"Horizontal then Vertical\", \"Vertical then Horizontal\"\n",
    "]\n",
    "\n",
    "[ax.set_title(title) for (ax, title) in zip(axes, titles)]\n",
    "[ax.set_axis_off() for ax in axes]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Kernel Separability</h3>\n",
    "<p>Notice how the last three results (filtered_hv0, filtered_hv1, filtered_hv2) are identical! This demonstrates an important concept called <strong>kernel separability</strong>.</p>\n",
    "\n",
    "<p>A 2D kernel is separable if it can be expressed as the outer product of two 1D kernels. For example, a 2D box filter can be separated into a horizontal and a vertical 1D box filter.</p>\n",
    "\n",
    "<p>Why is this important? Because it dramatically reduces computational complexity:</p>\n",
    "<ul>\n",
    "    <li>Applying a KxK kernel directly requires K² multiplications per pixel</li>\n",
    "    <li>Applying two 1D kernels of length K requires only 2K multiplications per pixel</li>\n",
    "</ul>\n",
    "\n",
    "<p>For large kernels, this can lead to significant performance improvements.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 3.1</h4>\n",
    "The goal here is to get a rough idea of the gain in computation obtained when using the kernel separability property on a convolution task.<br>\n",
    "Determine the complexity involved when carrying out a convolution using a <code>KxK</code> kernel on an image with N pixels. You can consider the image has zero padding on the borders to accommodate for the kernel size. Compare this complexity to the one needed to carry out a <code>1xK</code> convolution followed by a <code>Kx1</code>.<br>\n",
    "Compute the number of multiplications needed to carry out the blur filtering of the fish image (remember we used <code>K=31</code>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Computational Complexity</h4>\n",
    "<p>For each output pixel in a convolution:</p>\n",
    "<ul>\n",
    "    <li>A KxK kernel requires K² multiplications and additions</li>\n",
    "    <li>Two 1D kernels (K×1 and 1×K) require K + K = 2K operations</li>\n",
    "</ul>\n",
    "<p>For an image with N pixels, multiply these numbers by N to get the total operations.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the computational complexity for Ki = 31\n",
    "Ki = 31\n",
    "nb_pixels = np.prod(fish.shape)  # Total number of pixels in the image\n",
    "\n",
    "# For a KxK kernel applied to an image with N pixels:\n",
    "\n",
    "# For two separable kernels (1xK followed by Kx1):\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(f\"Image dimensions: {fish.shape}\")\n",
    "print(f\"Total pixels: {nb_pixels}\")\n",
    "print(f\"Kernel size: {Ki}x{Ki}\")\n",
    "print(\"\\nNumber of multiplications needed:\")\n",
    "print(f\" - Using a {Ki}x{Ki} kernel directly: {mults_2d_kernel:,}\")\n",
    "print(f\" - Using two separable {Ki}x1 and 1x{Ki} kernels: {mults_separable:,}\")\n",
    "print(f\"\\nReduction in computations: {reduction_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Computational Efficiency of Separable Kernels</h3>\n",
    "<p>As we can see from the calculations, using separable kernels reduces the number of operations by approximately 93.5% for a 31×31 kernel! This is why many image processing libraries automatically detect and optimize for separable kernels.</p>\n",
    "\n",
    "<p>Not all kernels are separable, but many common ones are, including:</p>\n",
    "<ul>\n",
    "    <li>Box filters (uniform averaging)</li>\n",
    "    <li>Gaussian filters</li>\n",
    "    <li>Some derivative filters (Sobel, etc.)</li>\n",
    "</ul>\n",
    "\n",
    "<p>This optimization becomes increasingly important as kernel size grows or when processing high-resolution images or video in real-time applications.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 High Pass Filtering\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding High Pass Filters</h3>\n",
    "<p>While low pass filters preserve low-frequency components (smooth areas) and remove high-frequency components (details), high pass filters do the opposite: they preserve high-frequency components and remove low-frequency components.</p>\n",
    "\n",
    "<p>High pass filters emphasize:</p>\n",
    "<ul>\n",
    "    <li>Edges and boundaries</li>\n",
    "    <li>Fine details and textures</li>\n",
    "    <li>Rapid changes in intensity</li>\n",
    "</ul>\n",
    "\n",
    "<p>A simple way to create a high pass filter is to subtract a low pass filtered image from the original image. This isolates the high-frequency components that were removed by the low pass filter.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the green channel to floating point and normalize to [0, 1] range\n",
    "f_fish = fish[:,:,1].astype(float) / 255\n",
    "\n",
    "# Apply and display a high pass filtered image here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Interpreting the High Pass Result</h3>\n",
    "<p>In the high pass filtered image:</p>\n",
    "<ul>\n",
    "    <li><strong>Values near zero (white/light colors)</strong>: Areas where pixel values are similar to their neighbors (smooth regions)</li>\n",
    "    <li><strong>Positive values (red)</strong>: Areas where pixel values are higher than their neighbors</li>\n",
    "    <li><strong>Negative values (blue)</strong>: Areas where pixel values are lower than their neighbors</li>\n",
    "</ul>\n",
    "\n",
    "<p>The high pass filter essentially shows where changes occur in the image. This is why high pass filters are often used for edge detection, as edges represent rapid changes in pixel values.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Edge Detection\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Edge Detection</h3>\n",
    "<p>Edge detection is one of the most fundamental operations in computer vision. Edges are areas with strong intensity contrasts – a significant change in pixel values over a relatively short distance.</p>\n",
    "\n",
    "<p>Edges typically correspond to:</p>\n",
    "<ul>\n",
    "    <li>Boundaries between objects</li>\n",
    "    <li>Boundaries between different surfaces within an object</li>\n",
    "    <li>Details within an object</li>\n",
    "    <li>Shadows or changes in illumination</li>\n",
    "</ul>\n",
    "\n",
    "<p>The simplest edge detectors are based on gradient filters, which measure the rate of change of pixel values in different directions.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple gradient kernel for edge detection\n",
    "K = np.array([[-1, 0, 1]], dtype=float).T  # Vertical gradient detector\n",
    "\n",
    "print(f\"Vertical gradient kernel K=\\n{K}\\n\")\n",
    "print(f\"Horizontal gradient kernel K.T=\\n{K.T}\\n\")\n",
    "print(f\"Combined 2D kernel K.dot(K.T)=\\n{K.dot(K.T)}\\n\")\n",
    "\n",
    "# Apply vertical gradient filter (detects horizontal edges)\n",
    "\n",
    "\n",
    "# Apply horizontal gradient filter (detects vertical edges)\n",
    "\n",
    "\n",
    "# Apply combined filter (detects edges in all directions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>How Gradient-Based Edge Detection Works</h3>\n",
    "<p>The gradient kernels we used are very simple implementations of the first derivative. They measure how quickly pixel values change in a particular direction:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>The vertical kernel <code>[[-1], [0], [1]]</code> detects horizontal edges by measuring changes in the vertical direction</li>\n",
    "    <li>The horizontal kernel <code>[[-1, 0, 1]]</code> detects vertical edges by measuring changes in the horizontal direction</li>\n",
    "</ul>\n",
    "\n",
    "<p>In areas where pixel values change rapidly, the filter produces large positive or negative values. In areas with constant intensity, it produces values close to zero.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge detection is a core component in computer vision. You are encouraged to have a look at the following resources if you want to go deeper on this topic:\n",
    "- [Sobel filtering](https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "- [Laplacian](https://en.wikipedia.org/wiki/Discrete_Laplace_operator#Image_processing)\n",
    "- [Canny edge detector](https://docs.opencv.org/4.0.0/da/d22/tutorial_py_canny.html)\n",
    "- [Difference of gaussian](https://en.wikipedia.org/wiki/Difference_of_Gaussians)\n",
    "\n",
    "Most of these filters have been implemented in OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore more advanced edge detection methods\n",
    "\n",
    "# Convert the fish image to grayscale for edge detection\n",
    "gray_fish = cv2.cvtColor(fish, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Apply Sobel edge detection\n",
    "sobelx = cv2.Sobel(gray_fish, cv2.CV_64F, 1, 0, ksize=3)  # x direction\n",
    "sobely = cv2.Sobel(gray_fish, cv2.CV_64F, 0, 1, ksize=3)  # y direction\n",
    "sobel_magnitude = np.sqrt(sobelx**2 + sobely**2)  # Combined magnitude\n",
    "sobel_magnitude = cv2.normalize(sobel_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "# Apply Laplacian edge detection\n",
    "laplacian = cv2.Laplacian(gray_fish, cv2.CV_64F)\n",
    "laplacian = cv2.normalize(laplacian, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "canny = cv2.Canny(gray_fish, 100, 200)  # Thresholds can be adjusted\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].imshow(gray_fish, cmap='gray')\n",
    "axes[0].set_title('Original Grayscale Image')\n",
    "\n",
    "axes[1].imshow(sobel_magnitude, cmap='gray')\n",
    "axes[1].set_title('Sobel Edge Detection')\n",
    "\n",
    "axes[2].imshow(laplacian, cmap='gray')\n",
    "axes[2].set_title('Laplacian Edge Detection')\n",
    "\n",
    "axes[3].imshow(canny, cmap='gray')\n",
    "axes[3].set_title('Canny Edge Detection')\n",
    "\n",
    "[ax.set_axis_off() for ax in axes]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pitfall-box\">\n",
    "<h4>Edge Detection Challenges</h4>\n",
    "<p>Simple gradient-based edge detectors have several limitations:</p>\n",
    "<ul>\n",
    "    <li><strong>Noise sensitivity</strong>: They can detect noise as edges</li>\n",
    "    <li><strong>Thickness</strong>: They often produce thick edges that need to be thinned</li>\n",
    "    <li><strong>Discontinuities</strong>: They may produce broken or incomplete edges</li>\n",
    "</ul>\n",
    "<p>More advanced edge detectors like Canny address these issues, but they also have more parameters to tune.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Median Filtering\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Median Filtering</h3>\n",
    "<p>Unlike the previous filters that use weighted averages (linear operations), median filtering is a non-linear operation that replaces each pixel with the <strong>median</strong> value of its neighborhood.</p>\n",
    "\n",
    "<p>Key properties of median filters:</p>\n",
    "<ul>\n",
    "    <li><strong>Edge preservation</strong>: Unlike mean filters, median filters preserve edges while removing noise</li>\n",
    "    <li><strong>Salt-and-pepper noise removal</strong>: Extremely effective at removing impulse noise (isolated pixels with very high or low values)</li>\n",
    "    <li><strong>Detail removal</strong>: Small details smaller than half the filter size tend to be removed</li>\n",
    "</ul>\n",
    "\n",
    "<p>These properties make median filters particularly useful for denoising images while preserving important structural features.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply median filter to the fish image\n",
    "median_blur = cv2.medianBlur(fish, 21)  # The second parameter is the kernel size (must be odd)\n",
    "\n",
    "# Display the result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(fish)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].set_axis_off()\n",
    "\n",
    "axes[1].imshow(median_blur)\n",
    "axes[1].set_title(\"Median Filtered Image (kernel size = 21)\")\n",
    "axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Toonify Effect\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Creating a Cartoon Effect</h3>\n",
    "<p>The \"toonify\" or cartoon effect is a popular image processing technique that makes photographs look like cartoons or drawings. The basic approach combines two main operations:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Detail Reduction</strong>: Simplify the image by reducing details while preserving edges, typically using median filtering or bilateral filtering</li>\n",
    "    <li><strong>Edge Enhancement</strong>: Detect and emphasize edges to create the characteristic cartoon outlines</li>\n",
    "</ol>\n",
    "\n",
    "<p>This combination creates the distinctive cartoon look with flat color regions separated by strong edges.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 3.2</h4>\n",
    "The cartoonish \"toonify\" effect can be obtained by combining the result of a median filtering with an edge detection. Try to implement it yourself!<br>\n",
    "<img src=\"./nb_data/toony_fish.jpg\" alt=\"toony fish\" style=\"margin:10px; width:200px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"hint-box\">\n",
    "<h4>Hint: Toonify Implementation</h4>\n",
    "<p>To create a toonify effect, you can follow these steps:</p>\n",
    "<ol>\n",
    "    <li>Apply median filtering to simplify the image while preserving edges</li>\n",
    "    <li>Use an edge detection algorithm (like Canny) to find edges</li>\n",
    "    <li>Optionally, thicken the edges using dilation</li>\n",
    "    <li>Combine the edges with the median-filtered image</li>\n",
    "</ol>\n",
    "<p>OpenCV's <code>cv2.Canny()</code> function is particularly useful for the edge detection step.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toonify(image):\n",
    "    \"\"\" Apply a cartoon effect to an image\n",
    "    \n",
    "    Arguments:\n",
    "        image (np.ndarray): The input image\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The cartoon-style image\n",
    "    \"\"\"\n",
    "    # Step 1: Apply median blur to simplify the image while preserving edges\n",
    "    \n",
    "    \n",
    "    # Step 2: Detect edges using Canny edge detector\n",
    "    \n",
    "    # Step 3: Make the edges a bit thicker for better visibility\n",
    "    \n",
    "    # Step 4. Copier l'image floutée pour appliquer le masque de bords\n",
    "    \n",
    "    # Step 5. Pour chaque pixel où un bord est détecté (valeur 255), on met le pixel à 0 (noir)\n",
    "\n",
    "    return toon_img\n",
    "\n",
    "toony_fish = toonify(flashy_fish)\n",
    "display_image(toony_fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section summary: Image Filtering and Convolutions</h3>\n",
    "<p>In this section, we've explored various image filtering techniques using convolutions:</p>\n",
    "<ul>\n",
    "    <li><strong>Low Pass Filters</strong>: Blur images by removing high-frequency components</li>\n",
    "    <li><strong>High Pass Filters</strong>: Enhance details by removing low-frequency components</li>\n",
    "    <li><strong>Edge Detection</strong>: Identify boundaries using gradient-based filters</li>\n",
    "    <li><strong>Median Filtering</strong>: Remove noise while preserving edges using non-linear filtering</li>\n",
    "    <li><strong>Toonify Effect</strong>: Example of \"effect\" obtained by combining filtering techniques</li>\n",
    "</ul>\n",
    "<p>We've also learned about important concepts like kernel separability, which can dramatically improve computational efficiency.</p>\n",
    "<p>These filtering techniques form the foundation for more advanced image processing and computer vision algorithms, including feature extraction, object detection, and image segmentation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Manipulations\n",
    "\n",
    "In this section, we'll explore various spatial transformations that can be applied to images. These transformations change the geometric properties of an image without modifying its pixel values.\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>What are Spatial Transformations?</h3>\n",
    "<p>Spatial transformations modify the spatial arrangement of pixels in an image. Unlike pixel value manipulations, these operations change the image's geometry:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Resizing</strong>: Changing the image's width and height</li>\n",
    "    <li><strong>Rotation</strong>: Turning the image around its center</li>\n",
    "    <li><strong>Translation</strong>: Shifting the image's position</li>\n",
    "    <li><strong>Scaling</strong>: Enlarging or reducing the image size</li>\n",
    "    <li><strong>Affine Transformations</strong>: More complex geometric changes that preserve parallel lines</li>\n",
    "    <li><strong>Perspective Transformations</strong>: Changing the view of the image</li>\n",
    "</ul>\n",
    "\n",
    "<p>These transformations are crucial in computer vision for tasks like image alignment, augmentation, and preprocessing.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Resizing an Image\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Image Resizing</h3>\n",
    "<p>Image resizing changes the number of pixels in an image, which can be done for various reasons:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Reducing image size to save storage or bandwidth</li>\n",
    "    <li>Enlarging images for display or analysis</li>\n",
    "    <li>Preparing images for machine learning models with fixed input sizes</li>\n",
    "    <li>Creating thumbnails</li>\n",
    "</ul>\n",
    "\n",
    "<p>OpenCV provides multiple interpolation methods for resizing, each with different characteristics:</p>\n",
    "<ul>\n",
    "    <li><strong>INTER_NEAREST</strong>: Fastest, but lowest quality</li>\n",
    "    <li><strong>INTER_LINEAR</strong>: Bilinear interpolation (default)</li>\n",
    "    <li><strong>INTER_CUBIC</strong>: Bicubic interpolation, smoother results</li>\n",
    "    <li><strong>INTER_LANCZOS4</strong>: High-quality resampling, computationally expensive</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling factor\n",
    "scalefactor = 5\n",
    "\n",
    "# Resize the image using different methods\n",
    "resized_nearest = cv2.resize(fish, None, fx=scalefactor, fy=scalefactor, interpolation=cv2.INTER_NEAREST)\n",
    "resized_linear = cv2.resize(fish, None, fx=scalefactor, fy=scalefactor, interpolation=cv2.INTER_LINEAR)\n",
    "resized_cubic = cv2.resize(fish, None, fx=scalefactor, fy=scalefactor, interpolation=cv2.INTER_CUBIC)\n",
    "resized_lanczos = cv2.resize(fish, None, fx=scalefactor, fy=scalefactor, interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].imshow(fish)\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "axes[1].imshow(resized_nearest)\n",
    "axes[1].set_title(f'Nearest (x{scalefactor})')\n",
    "\n",
    "axes[2].imshow(resized_linear)\n",
    "axes[2].set_title(f'Linear (x{scalefactor})')\n",
    "\n",
    "axes[3].imshow(resized_cubic)\n",
    "axes[3].set_title(f'Cubic (x{scalefactor})')\n",
    "\n",
    "axes[4].imshow(resized_lanczos)\n",
    "axes[4].set_title(f'Lanczos (x{scalefactor})')\n",
    "\n",
    "axes[5].axis('off')  # Remove the last subplot\n",
    "\n",
    "[ax.set_axis_off() for ax in axes[:5]]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Interpolation Methods Comparison</h3>\n",
    "<p>Notice the differences in image quality between interpolation methods (you may want to open the image with opencv and zoom in to see better):</p>\n",
    "<ul>\n",
    "    <li><strong>Nearest</strong>: Blocky, pixelated appearance</li>\n",
    "    <li><strong>Linear</strong>: Smoother, but can look slightly blurry</li>\n",
    "    <li><strong>Cubic</strong>: Smoother edges, better preservation of details</li>\n",
    "    <li><strong>Lanczos</strong>: Highest quality, best detail preservation</li>\n",
    "</ul>\n",
    "\n",
    "<p>Choose the interpolation method based on your specific requirements for image quality and computational efficiency.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Rotating an Image\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Image Rotation</h3>\n",
    "<p>Image rotation transforms an image by turning it around a specified point (usually the center). Key parameters include:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Rotation Angle</strong>: Degrees of rotation (positive for counterclockwise, negative for clockwise)</li>\n",
    "    <li><strong>Rotation Center</strong>: Point around which the image rotates</li>\n",
    "    <li><strong>Scale Factor</strong>: Optional scaling during rotation</li>\n",
    "</ul>\n",
    "\n",
    "<p>OpenCV uses the <code>cv2.getRotationMatrix2D()</code> function to compute the rotation matrix and <code>cv2.warpAffine()</code> to apply the transformation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation parameters\n",
    "rows, cols, _ = glasses.shape\n",
    "\n",
    "# Rotation angle and scale\n",
    "theta = 45.0  # Rotation angle in degrees\n",
    "scale_factor = 0.5  # Optional scaling\n",
    "\n",
    "# Compute rotation matrix\n",
    "rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), theta, scale_factor)\n",
    "\n",
    "# Apply rotation\n",
    "rotated_img = cv2.warpAffine(glasses, rotation_matrix, (cols, rows), borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(glasses)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].set_axis_off()\n",
    "\n",
    "axes[1].imshow(rotated_img)\n",
    "axes[1].set_title(f'Rotated {theta}° (scale: {scale_factor})')\n",
    "axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Rotation Matrix Breakdown</h3>\n",
    "<p>The rotation matrix combines two transformations:</p>\n",
    "<ul>\n",
    "    <li>Rotation around the center point</li>\n",
    "    <li>Optional scaling</li>\n",
    "</ul>\n",
    "\n",
    "<p>The matrix is computed using trigonometric functions to rotate points around the origin, then translated back to the center of the image.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Affine Transformations\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Affine Transformations</h3>\n",
    "<p>An affine transformation is a linear mapping method that preserves lines and parallelism. It can include:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Translation</li>\n",
    "    <li>Rotation</li>\n",
    "    <li>Scaling</li>\n",
    "    <li>Shearing (Skewing)</li> \n",
    "</ul>\n",
    "\n",
    "<p>Unlike perspective transformations, affine transformations do not change the relative distances between points.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation).\n",
    " \n",
    "[It can easily be computed in OpenCV](https://theailearner.com/tag/cv2-getaffinetransform/) using the `cv2.getAffineTransform()` and `cv2.warpAffine()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for affine transformation\n",
    "rows, cols, ch = glasses.shape\n",
    "\n",
    "# Define source and target points for the affine transformation\n",
    "source = np.float32([[0, 0], [cols, 0], [0, rows]])\n",
    "target = np.float32([[10, 10], [cols-100, 80], [5, rows]])\n",
    "\n",
    "# Compute the affine transformation matrix\n",
    "matrix = cv2.getAffineTransform(source, target)\n",
    "\n",
    "# Apply the transformation\n",
    "glasses_transformed = cv2.warpAffine(glasses, matrix, (cols, rows))\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(glasses)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].scatter(source[:,0], source[:,1], color='cyan', label='Source Points')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].imshow(glasses_transformed)\n",
    "axes[1].set_title('Affine Transformed Image')\n",
    "axes[1].scatter(target[:,0], target[:,1], color='red', label='Target Points')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Affine Transformation Mechanics</h3>\n",
    "<p>To compute an affine transformation, you need:</p>\n",
    "<ol>\n",
    "    <li>Three corresponding points in the source image</li>\n",
    "    <li>Three corresponding points in the target image</li>\n",
    "</ol>\n",
    "\n",
    "<p>OpenCV calculates a 2x3 transformation matrix that maps the source points to the target points, preserving parallel lines.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Perspective Transformations\n",
    "\n",
    "Here is a definition of another type of transformation: the [perspective transformation](https://en.wikipedia.org/wiki/Transformation_matrix#Perspective_projection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Perspective Transformations</h3>\n",
    "<p>Perspective transformations change the view of an image, mapping a quadrilateral to another quadrilateral. Unlike affine transformations, they do not preserve parallel lines.</p>\n",
    "\n",
    "<p>Common applications include:</p>\n",
    "<ul>\n",
    "    <li>Image rectification</li>\n",
    "    <li>Document scanning</li>\n",
    "    <li>Augmented reality</li>\n",
    "    <li>Camera calibration</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for perspective transformation\n",
    "rows, cols, chans = glasses.shape\n",
    "\n",
    "# Define source and target points for the perspective transformation\n",
    "source = np.float32([[0, 0], [cols, 0], [cols, rows], [0, rows]])\n",
    "target = np.float32([[0, 0], [cols/2, 20], [cols/3, 300], [20, 200]])\n",
    "\n",
    "# Compute the perspective transformation matrix\n",
    "matrix = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "# Apply the transformation\n",
    "glasses_transformed = cv2.warpPerspective(glasses, matrix, (cols, rows))\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(glasses)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].scatter(source[:,0], source[:,1], color='cyan', label='Source Points')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].imshow(glasses_transformed)\n",
    "axes[1].set_title('Perspective Transformed Image')\n",
    "axes[1].scatter(target[:,0], target[:,1], color='red', label='Target Points')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-fish-related example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/right.jpg')\n",
    "rows, cols, chans = img.shape\n",
    "\n",
    "source =  np.float32([[362, 107], [530, 139] , [453, 416], [319, 337]])\n",
    "target =  np.float32([[100, 100], [330, 100],  [330, 400], [100, 400]])\n",
    "\n",
    "matrix = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "img_transformed = cv2.warpPerspective(img, matrix, (rows, cols))\n",
    "\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.scatter(source[:,0], source[:,1], color='c')\n",
    "plt.scatter(target[:,0], target[:,1], color='r', marker='*')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_transformed[100:400, 100:330])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Perspective Transformation Mechanics</h3>\n",
    "<p>To compute a perspective transformation, you need:</p>\n",
    "<ol>\n",
    "    <li>Four corresponding points in the source image</li>\n",
    "    <li>Four corresponding points in the target image</li>\n",
    "</ol>\n",
    "\n",
    "<p>OpenCV calculates a 3x3 transformation matrix that maps the source quadrilateral to the target quadrilateral, allowing for complex view changes.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 4.1</h4>\n",
    "Map the glasses to the head of the flashy fish and create a <code>composite_image()</code> that looks something like the image shown below.<br>\n",
    "<strong>Bonus:</strong> use the alpha channel to keep the transparency information for the lenses.<br>\n",
    "<img src=\"./nb_data/classy_fish.jpg\" alt=\"classy fish\" style=\"margin:10px; width:200px\">\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{rgb_glasses.shape = }\")\n",
    "print(f\"{flashy_fish.shape = }\")\n",
    "\n",
    "rows, cols, _ = rgb_glasses.shape\n",
    "final_rows, final_cols, _ = flashy_fish.shape\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "classy_fish = composite_image(flashy_fish, warped_glasses, warped_alpha)\n",
    "\n",
    "# Display the composit image\n",
    "display_image(classy_fish)\n",
    "# plt.imsave(\"./nb_data/classy_fish.jpg\", classy_fish)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Advanced Transformations\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Arbitrary Image Warping</h3>\n",
    "<p>Beyond standard transformations, OpenCV allows for complex, non-linear image distortions using displacement maps. These can create interesting visual effects or correct lens distortions.</p>\n",
    "\n",
    "<p>Key techniques include:</p>\n",
    "<ul>\n",
    "    <li>Radial distortion</li>\n",
    "    <li>Barrel and pincushion transformations</li>\n",
    "    <li>Lens correction</li>\n",
    "    <li>Creative image warping</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know you can use [OpenCV displacement maps](https://docs.opencv.org/4.x/d1/da0/tutorial_remap.html) to distort images any way you like, for example, you can apply a radial dilatation or contraction to your image for some funny looking results (...if that's the kind of humour you like).<br>\n",
    "<img src=\"./nb_data/big_nose_fish.jpg\" alt=\"Big nose fish\" style=\"margin:10px; width:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<h4>Exercise 4.2</h4>\n",
    "\n",
    "Propose a centered non parametric radial distortion implementation.<br>\n",
    "You can also try to implement one with a parametric center, radius and dilatation factor and use it to make a \"big eyed fish\".\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_dilatation(image):\n",
    "    transformed_image = image.copy()\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    xx, yy = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    xx = xx.astype(np.float32) - w/2\n",
    "    yy = yy.astype(np.float32) - h/2\n",
    "\n",
    "    # your code starts here \n",
    "    # (you can choose to follow the different steps that are provided or try something of your own)\n",
    "\n",
    "    # step #1: polar transformation\n",
    "    \n",
    "\n",
    "    # step #2: radial distorsion (NB: keep the radius in the range [0, Rmax])\n",
    "    \n",
    "\n",
    "    # step #3: map back to cartesian space\n",
    "\n",
    "    cv2.remap()\n",
    "    \n",
    "    # your code ends here\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "display_image(radial_dilatation(fish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [0 1 2 3]\n",
      " [0 1 2 3]]\n",
      "[[0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [2 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "xx, yy = np.meshgrid(np.arange(4), np.arange(3))\n",
    "print(xx)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Radial Distortion Mechanics</h3>\n",
    "<p>The radial distortion function works by:</p>\n",
    "<ol>\n",
    "    <li>Creating a coordinate grid centered at the image's center</li>\n",
    "    <li>Computing the radial distance from the center</li>\n",
    "    <li>Scaling coordinates based on their radial distance</li>\n",
    "    <li>Remapping the image using the scaled coordinates</li>\n",
    "</ol>\n",
    "\n",
    "<p>By adjusting the strength parameter, you can create various distortion effects:</p>\n",
    "<ul>\n",
    "    <li>Positive values create a \"bulge\" or \"fish-eye\" effect</li>\n",
    "    <li>Negative values create a \"pinch\" or \"inverse fish-eye\" effect</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section 4 Summary: Spatial Manipulations</h3>\n",
    "<p>In this section, we've explored various ways to transform the spatial arrangement of pixels in an image:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Resizing</strong>: Changing the dimensions of an image while preserving its content</li>\n",
    "    <li><strong>Rotation</strong>: Turning an image around its center or a specified point</li>\n",
    "    <li><strong>Affine Transformations</strong>: Linear transformations that preserve parallel lines</li>\n",
    "    <li><strong>Perspective Transformations</strong>: More complex transformations that map quadrilaterals to quadrilaterals</li>\n",
    "    <li><strong>Advanced Transformations</strong>: Non-linear distortions for creative effects or lens correction</li>\n",
    "</ul>\n",
    "\n",
    "<p>These spatial transformations are essential for many computer vision tasks, including:</p>\n",
    "<ul>\n",
    "    <li>Preparing images for analysis (standardizing size and orientation)</li>\n",
    "    <li>Correcting for camera perspective and lens distortion</li>\n",
    "    <li>Creating special effects for creative applications</li>\n",
    "    <li>Aligning images for comparison or stitching</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Miscellaneous (but important) Techniques\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Beyond the Basics</h3>\n",
    "<p>This section presents additional tools and algorithms that didn't fit neatly into the previous sections but can be valuable additions to your computer vision toolkit. These techniques are commonly used in practical applications and can help solve specific image processing challenges.</p>\n",
    "</div>\n",
    "\n",
    "### 5.1 Thresholding\n",
    "\n",
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Thresholding</h3>\n",
    "<p>Thresholding is one of the simplest yet most powerful segmentation techniques in image processing. It separates pixels in an image into two categories based on their intensity values:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Pixels with values above a threshold are set to one value (often white)</li>\n",
    "    <li>Pixels with values below the threshold are set to another value (often black)</li>\n",
    "</ul>\n",
    "\n",
    "<p>This creates a binary image that highlights features of interest and simplifies subsequent analysis.</p>\n",
    "\n",
    "<p>There are several types of thresholding:</p>\n",
    "<ul>\n",
    "    <li><strong>Global thresholding</strong>: Uses a single threshold value for the entire image</li>\n",
    "    <li><strong>Adaptive thresholding</strong>: Calculates different thresholds for different regions of the image</li>\n",
    "    <li><strong>Otsu's method</strong>: Try to automatically determines the optimal threshold by minimizing intra-class variance</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the naive version to the adaptative one on a mediocre image of a Sudoku grid (this is inspired from a classic [OpenCV example](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a grayscale image of a Sudoku puzzle\n",
    "img = cv2.imread('data/sudokubig.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.medianBlur(img, 5)  # Apply median blur to reduce noise\n",
    "\n",
    "# Apply different thresholding techniques\n",
    "# 1. Global thresholding\n",
    "th_value, th_image_01 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# 2. Adaptive thresholding with mean\n",
    "th_image_02 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                    cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# 3. Adaptive thresholding with Gaussian\n",
    "th_image_03 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                    cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# 4. Otsu's thresholding\n",
    "_, th_image_04 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(img, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "axes[1].imshow(th_image_01, cmap=\"gray\")\n",
    "axes[1].set_title(f\"Global Thresholding (value = {th_value})\")\n",
    "\n",
    "axes[2].imshow(th_image_02, cmap=\"gray\")\n",
    "axes[2].set_title(\"Adaptive Mean Thresholding\")\n",
    "\n",
    "axes[3].imshow(th_image_03, cmap=\"gray\")\n",
    "axes[3].set_title(\"Adaptive Gaussian Thresholding\")\n",
    "\n",
    "axes[4].imshow(th_image_04, cmap=\"gray\")\n",
    "axes[4].set_title(\"Otsu's Thresholding\")\n",
    "\n",
    "[ax.set_axis_off() for ax in axes]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Comparing Thresholding Methods</h3>\n",
    "<p>As you can see from the results:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Global thresholding</strong> applies a single threshold to the entire image. This works well for images with uniform lighting and good contrast between foreground and background.</li>\n",
    "    <li><strong>Adaptive thresholding</strong> calculates thresholds for small regions of the image, making it more robust to lighting variations across the image.</li>\n",
    "    <li>The <strong>mean method</strong> uses the mean of the neighborhood area as the threshold.</li>\n",
    "    <li>The <strong>Gaussian method</strong> uses a weighted mean, where weights are a Gaussian window.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Adaptive methods typically perform better on images with varying illumination, such as the Sudoku grid shown here.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Morphological Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Understanding Morphological Operations</h3>\n",
    "<p>Morphological operations are a set of non-linear operations that process images based on shapes. They apply a structuring element to an input image, creating an output image of the same size.</p>\n",
    "\n",
    "<p>These operations are typically performed on binary images but can also be applied to grayscale images.</p>\n",
    "\n",
    "<p>Common morphological operations include:</p>\n",
    "<ul>\n",
    "    <li><strong>Erosion</strong>: Shrinks objects in the foreground (white regions in a binary image)</li>\n",
    "    <li><strong>Dilation</strong>: Expands objects in the foreground</li>\n",
    "    <li><strong>Opening</strong>: Erosion followed by dilation, useful for removing noise</li>\n",
    "    <li><strong>Closing</strong>: Dilation followed by erosion, useful for closing small holes</li>\n",
    "    <li><strong>Flood Fill</strong>: Fills enclosed regions with a specified value</li>\n",
    "</ul>\n",
    "\n",
    "<p>These operations are fundamental in image preprocessing, segmentation, and feature extraction.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the way to go before deep learning took off! They remain nonetheless very efficient and simple to manipulate. They are mainly used to manipulate binary masks, these masks are then used to carry out filtering operations on the original image.\n",
    "\n",
    "Here is a nice OpenCV tutorial to get a better idea of what's possible: https://docs.opencv.org/4.0.0/d9/d61/tutorial_py_morphological_ops.html#gsc.tab=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple binary image (a doughnut shape)\n",
    "yy = np.mgrid[-1:1:0.01, -1:1:0.01][0]\n",
    "xx = yy.T\n",
    "doughnut = np.zeros_like(xx)\n",
    "doughnut[xx**2 + yy**2 < 0.5**2] = 1  # Outer circle\n",
    "doughnut[xx**2 + yy**2 < 0.2**2] = 0  # Inner circle (hole)\n",
    "\n",
    "# Define a kernel for morphological operations\n",
    "kernel = np.ones((10, 10), np.float32)\n",
    "\n",
    "# Apply different morphological operations\n",
    "erosion = cv2.erode(doughnut, kernel, iterations=2)\n",
    "dilation = cv2.dilate(doughnut, kernel, iterations=2)\n",
    "# For flood fill, we need to convert to uint8 and make a copy\n",
    "doughnut_uint8 = doughnut.astype(np.uint8)\n",
    "floodfilled = doughnut_uint8.copy()\n",
    "mask = np.zeros((floodfilled.shape[0]+2, floodfilled.shape[1]+2), np.uint8)\n",
    "cv2.floodFill(floodfilled, mask, (100, 100), 1)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(doughnut, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original Doughnut\")\n",
    "\n",
    "axes[1].imshow(erosion, cmap=\"gray\")\n",
    "axes[1].set_title(\"Eroded Doughnut\")\n",
    "\n",
    "axes[2].imshow(dilation, cmap=\"gray\")\n",
    "axes[2].set_title(\"Dilated Doughnut\")\n",
    "\n",
    "axes[3].imshow(floodfilled, cmap=\"gray\")\n",
    "axes[3].set_title(\"Flood-Filled Doughnut\")\n",
    "\n",
    "[ax.set_axis_off() for ax in axes]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theory-box\">\n",
    "<h3>Practical Example: Selecting Image Regions</h3>\n",
    "<p>Let's apply morphological operations to a real image. We'll select the yellow area around the fish's eyes using color thresholding and morphological operations:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HSV color space for easier color selection\n",
    "h, s, v = split_channels(cv2.cvtColor(fish, cv2.COLOR_RGB2HSV))\n",
    "\n",
    "# Create a mask for yellow pixels based on their hue\n",
    "mask_yellow = np.logical_and(h > 20, h < 28).astype(np.uint8)\n",
    "\n",
    "# Apply morphological operations to clean up the mask\n",
    "kernel = np.ones((3, 3), np.float32)\n",
    "erosion = cv2.erode(mask_yellow, kernel, iterations=5)  # Remove small noise\n",
    "dilation = cv2.dilate(erosion, kernel, iterations=5)    # Expand the remaining regions\n",
    "\n",
    "# Modify the image based on the mask\n",
    "h_modified = h.copy()\n",
    "s_modified = s.copy()\n",
    "v_modified = v.copy()\n",
    "\n",
    "# Adjust hue, saturation, and value in the masked regions\n",
    "h_modified += 15 * dilation  # Shift hue\n",
    "s_modified += 100 * dilation  # Increase saturation\n",
    "v_modified += 50 * dilation   # Increase brightness\n",
    "\n",
    "# Convert back to RGB\n",
    "modified_hsv = recompose_channels([h_modified, s_modified, v_modified])\n",
    "eyebrowed_fish = cv2.cvtColor(modified_hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "axes[0].imshow(fish)\n",
    "axes[0].set_title(\"Original Fish\")\n",
    "\n",
    "axes[1].imshow(mask_yellow, cmap=\"gray\")\n",
    "axes[1].set_title(\"Yellow Area Mask\")\n",
    "\n",
    "axes[2].imshow(erosion, cmap=\"gray\")\n",
    "axes[2].set_title(\"Eroded Mask\")\n",
    "\n",
    "axes[3].imshow(dilation, cmap=\"gray\")\n",
    "axes[3].set_title(\"Opening (Erosion + Dilation)\")\n",
    "\n",
    "axes[4].imshow(eyebrowed_fish)\n",
    "axes[4].set_title(\"Modified Fish\")\n",
    "\n",
    "[ax.set_axis_off() for ax in axes]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary-box\">\n",
    "<h3>Section summary: Miscellaneous Techniques</h3>\n",
    "<p>In this section, we've explored additional image processing techniques that complement the methods covered in previous sections:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Thresholding</strong>: Converting grayscale images to binary images to highlight features of interest</li>\n",
    "    <li><strong>Morphological Operations</strong>: Shape-based processing to clean up images and extract features</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
